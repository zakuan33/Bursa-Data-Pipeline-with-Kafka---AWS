{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b7e1a0-7da9-486a-9694-e6d532fda8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in c:\\users\\telet\\anaconda3\\lib\\site-packages (2.2.15)\n",
      "Requirement already satisfied: s3fs in c:\\users\\telet\\anaconda3\\lib\\site-packages (2024.3.1)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from s3fs) (2.12.3)\n",
      "Requirement already satisfied: fsspec==2024.3.1 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from s3fs) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from s3fs) (3.9.5)\n",
      "Requirement already satisfied: botocore<1.34.70,>=1.34.41 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.69)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.14.1)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.7.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.9.3)\n",
      "Requirement already satisfied: typing_extensions>=3.7 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from aioitertools<1.0.0,>=0.5.1->aiobotocore<3.0.0,>=2.5.4->s3fs) (4.14.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.5.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\telet\\anaconda3\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.70,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a07abb68-51af-4d75-b5c6-1859fcc494ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kafka import KafkaConsumer\n",
    "from time import sleep\n",
    "from json import loads,dumps\n",
    "from s3fs import S3FileSystem\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "208abc78-dfc4-4711-b91a-0b6d614aa110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\noptional ->by default, AWS Glue Crawlers automatically lowercase column names when inferring schema from JSON files,which create\\nrows in Athena but the columns are blank, as files on s3 use a different casing to name the columns\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "KAFKA_IP = os.getenv(\"KAFKA_BROKER_IP\")\n",
    "BUCKET_NAME = os.getenv(\"S3_BUCKET_NAME\")\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    #'demo_test',\n",
    "    'bursaMarketShare',\n",
    "    bootstrap_servers=[KAFKA_IP],\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8')),\n",
    "    auto_offset_reset='earliest',\n",
    "    consumer_timeout_ms=5000  # stop after 5 seconds of inactivity\n",
    ")\n",
    "all_data = [msg.value for msg in consumer] \n",
    "#kafka lazy af , If no new messages are coming in, they sit idle forever which cause no message detected hence collect msg when it starts is better\n",
    "\n",
    "'''\n",
    "optional ->by default, AWS Glue Crawlers automatically lowercase column names when inferring schema from JSON files,which create\n",
    "rows in Athena but the columns are blank, as files on s3 use a different casing to name the columns\n",
    "'''\n",
    "#def lowercase_keys(d):\n",
    "    #return {k.lower(): v for k, v in d.items()}\n",
    "\n",
    "#lowered_data = [lowercase_keys(row) for row in all_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89c072df-ff31-41b8-93af-66c992fdb5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'No': 1, 'CASHTAG': '$SEVE', 'NAME': \"7-ELEVEN M'SIA\", 'PRICE': 1.99, 'CLOSE': 1.99, 'CHG': 0.0, '%CHG': '0.000%', 'VOLUME': 0, 'HIGH': 1.99, 'LOW': 1.99, 'BUY (QTY)': '1.980 (500)', 'SELL (QTY)': '2.000 (116900)'}, {'No': 3, 'CASHTAG': '$ARNK', 'NAME': 'A-RANK BHD', 'PRICE': 0.435, 'CLOSE': 0.44, 'CHG': 0.0, '%CHG': '0.000%', 'VOLUME': 0, 'HIGH': 0.45, 'LOW': 0.44, 'BUY (QTY)': '0.425 (4500)', 'SELL (QTY)': '0.480 (7000)'}, {'No': 5, 'CASHTAG': '$ABLE', 'NAME': 'ABLE GLOBAL', 'PRICE': 1.52, 'CLOSE': 1.53, 'CHG': -0.01, '%CHG': '-0.650%', 'VOLUME': 94000, 'HIGH': 1.52, 'LOW': 1.51, 'BUY (QTY)': '1.510 (59600)', 'SELL (QTY)': '1.520 (19000)'}, {'No': 8, 'CASHTAG': '$ACME', 'NAME': 'ACME HOLDGS BHD', 'PRICE': 0.125, 'CLOSE': 0.13, 'CHG': -0.005, '%CHG': '-3.850%', 'VOLUME': 11000, 'HIGH': 0.13, 'LOW': 0.13, 'BUY (QTY)': '0.125 (20000)', 'SELL (QTY)': '0.130 (500)'}, {'No': 10, 'CASHTAG': '$REN3', 'NAME': '3REN BERHAD', 'PRICE': 0.255, 'CLOSE': 0.25, 'CHG': 0.01, '%CHG': '4.080%', 'VOLUME': 2179100, 'HIGH': 0.26, 'LOW': 0.25, 'BUY (QTY)': '0.250 (495200)', 'SELL (QTY)': '0.255 (138100)'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print (message .value)\n",
    "print (all_data[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "822d1372-bfdf-4473-a5a9-aeb0589ca510",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3FileSystem()\n",
    "#Per-message write\n",
    "for count, i in enumerate(all_data):\n",
    "    with s3.open(f\"s3://{BUCKET_NAME}/bursamarketshare{count}.json\", 'w') as file:\n",
    "        json.dump(i, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6db91555-f268-4ab5-877b-6fda7cdd8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3FileSystem()\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\") \n",
    "\"\"\"\n",
    "crawler has issues with with mixed file formats so choose one dont be greedy\n",
    "\"\"\"\n",
    "#json\n",
    "file_name = f\"s3://{BUCKET_NAME}/bursamarketshare_{timestamp}.json\"\n",
    "#csv option\n",
    "#file_name = f\"s3://demotestzakuan/bursamarketshare_{timestamp}.csv\"\n",
    "df = pd.DataFrame(all_data)\n",
    "with s3.open(file_name, 'w') as file:\n",
    "    #json.dump(all_data[:5], file,indent=2)\n",
    "    #json.dump(all_data, file,indent=2) \n",
    "    #athena becomes dump cuz above line producing JSON with a top-level array, which Athena cannot query directly\n",
    "    \n",
    "    # Write NDJSON (newline-delimited JSON) file\n",
    "    df.to_json(file, orient='records', lines=True)\n",
    "\n",
    "    #csv option\n",
    "    #df.to_csv(file, index=False)\n",
    "#Batch process all messages into one file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5189a-fefa-4b90-ab3f-34991fa35149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
